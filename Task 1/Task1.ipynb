{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b44ee0f",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Semantic textual similarity deals with determining how similar a pair of text documents are. The goal of the first task is to implement a new architecture by combining the ideas from papers\n",
    "- Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al (will be referred as the AAAI paper)\n",
    "- A Structured Self-Attentive Sentence Embedding, Zhouhan Lin et. al (will be referred as the ICLR paper) <br/><br/>\n",
    "Furthermore, you'd be evaluating whether the new architecture improves the results of **Siamese Recurrent Architectures for Learning Sentence Similarity, Jonas Mueller et. al**. Your overall network architecture should look similar to the following figure. \n",
    "![Untitled%20Diagram.drawio%20%281%29.png](https://raw.githubusercontent.com/shahrukhx01/ocr-test/main/download.png)\n",
    "<br/><br/>\n",
    "\n",
    "\n",
    "Moreover, you'd be required to implement further helper functions that these papers propose i.e., attention penalty term for loss, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbed585c",
   "metadata": {},
   "source": [
    "### SICK dataset\n",
    "We will use SICK dataset throughout the project (at least in the first two tasks). To get more information about the dataset you can refer to the original [paper](http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf) on the dataset. You can download the dataset using one of the following links:\n",
    "- [dataset page 1](https://marcobaroni.org/composes/sick.html)\n",
    "- [dataset page 2](https://huggingface.co/datasets/sick)    \n",
    "\n",
    "The relevant columns for the project are `sentence_A`, `sentence_B`, `relatedness_score`, where `relatedness_score` is the label. <br><br>\n",
    "**Hint: For each task make sure to decide whether the label should be normalized or not.**<br><br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c132db85",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69b52c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from test import evaluate_test_set\n",
    "import sts_data\n",
    "import siamese_lstm_attention\n",
    "import train\n",
    "import test\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d3a701",
   "metadata": {},
   "source": [
    "## Part 1. Data pipeline (3 points)\n",
    "Before starting working on the model, we must configure the data pipeline to load the data in the correct format. Please, implement the functions for processing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9af7f4ee",
   "metadata": {},
   "source": [
    "### Part 1.1 Loading and preprocessing the data (1 point)\n",
    "Download the SICK dataset and store it in [pandas](https://pandas.pydata.org/docs/index.html) `Dataframe`'s. You should use the official data split.  \n",
    "\n",
    "Implement `load_data` method of `STSData` class in `sts_data.py`. The method must download the dataset and perform basic preprocessing. Minimal preprocessing required:  \n",
    "1. normalize text to lower case\n",
    "2. remove punctuations  \n",
    "3. remove [stopwords](https://en.wikipedia.org/wiki/Stop_word) - we provided you with the list of English stopwords.\n",
    "4. Optionally, any other preprocessing that you deem necessary.\n",
    "\n",
    "All the preprocessing code must be contained in the `preprocessing.py` file.  \n",
    "You can use Hugginface's [datasets library](https://huggingface.co/docs/datasets/) for easy dataset download."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd87f192",
   "metadata": {},
   "source": [
    "### Part 1.2 Building vocabulary (1 point)\n",
    "Before we can feed our text to the model it must be vectorized. We use 300 dimensional pretrained [FastText embeddings](https://fasttext.cc/docs/en/english-vectors.html) for mapping words to vectors. To know more general information about embeddings you can refer to [this video](https://www.youtube.com/watch?v=ERibwqs9p38) (even though, we use different types of embeddings - FastText vs Word2Vec described in the video - the general purpose of them is the same).  \n",
    "In order to apply the embedding, we must first construct the vocabulary for data. Complete the `create_vocab` method of `STSData` class in `sts_data.py` where you concatenate each sentence pair, tokenize it and construct the vocabulary for the whole training data. You should use [torchtext](https://torchtext.readthedocs.io/en/latest/data.html\n",
    ") for processing the data. For tokenization, you can use any library (or write your own tokenizer), but we recommend you to use tokenizer by [spacy](https://spacy.io/). Use the `fasttext.simple.300d` as pretrained vectors.  \n",
    "In the end, you must have a vocabulary object capable of mapping your input to corresponding vectors. Remember that the vocabulary is created using only training data (not touching validation or test data)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b90724d",
   "metadata": {},
   "source": [
    "### Part 1.3 Creating DataLoader (1 point)\n",
    "Implement `get_data_loader` method of `STSData` class in `sts_data.py`. It must perform the following operations on each of the data splits:\n",
    "1. vectorize each pair of the sentences by replacing all tokens with their index in vocabulary\n",
    "2. normalize labels\n",
    "3. convert everything to PyTorch tensors\n",
    "4. pad every sentence so that all of them have the same length\n",
    "5. create `STSDataset` from `dataset.py`\n",
    "6. create PyTorch DataLoader out of the created dataset. \n",
    "\n",
    "\n",
    "We have provided you with the interfaces of possible helper functions, but you can change them as you need.   \n",
    "In the end, you must have 3 data loaders for each of the splits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62b40225",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:loading and preprocessing data...\n",
      "WARNING:datasets.builder:Using custom data configuration default\n",
      "WARNING:datasets.builder:Reusing dataset sick (C:\\Users\\DELL\\.cache\\huggingface\\datasets\\sick\\default\\0.0.0\\c6b3b0b44eb84b134851396d6d464e5cb8f026960519d640e087fe33472626db)\n",
      "INFO:root:reading and preprocessing data completed...\n",
      "INFO:root:creating vocabulary...\n",
      "INFO:torchtext.vocab:Loading vectors from .vector_cache\\wiki.simple.vec.pt\n",
      "INFO:root:creating vocabulary completed...\n"
     ]
    }
   ],
   "source": [
    "reload(sts_data)\n",
    "from sts_data import STSData\n",
    "\n",
    "columns_mapping = {\n",
    "        \"sent1\": \"sentence_A\",\n",
    "        \"sent2\": \"sentence_B\",\n",
    "        \"label\": \"relatedness_score\",\n",
    "    }\n",
    "dataset_name = \"sick\"\n",
    "sick_data = STSData(\n",
    "    dataset_name=dataset_name,\n",
    "    columns_mapping=columns_mapping,\n",
    "    normalize_labels=True,\n",
    "    normalization_const=5.0,\n",
    ")\n",
    "batch_size = 64\n",
    "sick_dataloaders = sick_data.get_data_loader(batch_size=batch_size) # returns 3 dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98c6526",
   "metadata": {},
   "source": [
    "## Part 2. Model Configuration & Hyperparameter Tuning (3 points)\n",
    "In this part, you are required to define a model capable of learning self-attentive sentence embeddings described in [this ICLR paper](https://arxiv.org/pdf/1703.03130.pdf). The sentence embedding learned by this model will be used for computing the similarity score instead of the simpler embeddings described in the original AAAI paper.  \n",
    "Please familiarize yourself with the model described in the ICLR paper and implement `SiameseBiLSTMAttention` and `SelfAttention` classes in `siamese_lstm_attention.py`. Remember that you must run the model on each sentence in the sentence pair to calculate the similarity between them. You can use `similarity_score` from `utils.py` to compute the similarity score between two sentences. \n",
    "  \n",
    "To get more theoretical information about attention mechanisms you can refer to [this chapter](https://web.stanford.edu/~jurafsky/slp3/10.pdf) of [\"Speech and Language Processing\" book](https://web.stanford.edu/~jurafsky/slp3/) by Dan Jurafsky and James H. Martin, where the attention mechanism is described in the context of the machine translation task. \n",
    "\n",
    "Finally, once your implementation works on the default parameters stated below, make sure to perform **hyperparameter tuning** to find the best combination of hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a530ec28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters selected after hyperparam. tuning\n",
    "\n",
    "output_size = 1\n",
    "hidden_size = 128\n",
    "vocab_size = len(sick_data.vocab)\n",
    "embedding_size = 300\n",
    "embedding_weights = sick_data.vocab.vectors\n",
    "lstm_layers = 5\n",
    "learning_rate = 1e-2\n",
    "fc_hidden_size = 32\n",
    "max_epochs = 5\n",
    "bidirectional = True\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "## self attention config\n",
    "self_attention_config = {\n",
    "    \"hidden_size\": 350,  ## refers to variable 'da' in the ICLR paper\n",
    "    \"output_size\": 30,  ## refers to variable 'r' in the ICLR paper\n",
    "    \"penalty\": 1,  ## refers to penalty coefficient term in the ICLR paper\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a879b228",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SiameseBiLSTMAttention(\n",
       "  (embeddings): Embedding(2052, 300)\n",
       "  (bilstm): LSTM(300, 128, num_layers=5, bidirectional=True)\n",
       "  (W_s1): Linear(in_features=256, out_features=350, bias=True)\n",
       "  (dropout1): Dropout(p=0.1, inplace=False)\n",
       "  (W_s2): Linear(in_features=350, out_features=30, bias=True)\n",
       "  (fc_layer): Linear(in_features=7680, out_features=32, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#reload(siamese_lstm_attention)\n",
    "from siamese_lstm_attention import SiameseBiLSTMAttention\n",
    "## init siamese lstm\n",
    "siamese_lstm_attention_model = SiameseBiLSTMAttention(\n",
    "    batch_size=batch_size,\n",
    "    output_size=output_size,\n",
    "    hidden_size=hidden_size,\n",
    "    vocab_size=vocab_size,\n",
    "    embedding_size=embedding_size,\n",
    "    embedding_weights=embedding_weights,\n",
    "    lstm_layers=lstm_layers,\n",
    "    self_attention_config=self_attention_config,\n",
    "    fc_hidden_size=fc_hidden_size,\n",
    "    device=device,\n",
    "    bidirectional=bidirectional,\n",
    ")\n",
    "## move model to device\n",
    "siamese_lstm_attention_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b100de",
   "metadata": {},
   "source": [
    "## Part 3. Training (2 points)  \n",
    "Perform the final training of the model by implementing functions in `train.py` after setting values of your best-chosen hyperparameters. Note you can use the same training function when performing hyperparameter tuning.\n",
    "- **What is a good choice of performance metric here for evaluating your model?** [Max 2-3 lines]\n",
    "- **What other performance evaluation metric can we use here for this task? Motivate your answer.**[Max 2-3 lines]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4309b3af",
   "metadata": {},
   "source": [
    "### Part 3 Answers\n",
    "\n",
    "1) A good choice of performance metric is the mean squared error, as we are comparing the difference in relatedness scores of the 2 sentences which is a quantitative value. Hence, we have used (1 - MSE) over the normalised label values as the accuracy metric. Hence, the larger the (1 - MSE) value, the better is the accuracy. \n",
    "\n",
    "2) Another good performance evaluation metric could be the Pearson correlation coefficient. This metric lies between 0 and 1, and would predict value 1 for complete correlation between sentences, and value 0 for no correlation between sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "da5a31ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                            | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running EPOCH 1\n",
      "Running loss:  11.115558910369874\n",
      "Training set accuracy: 0.8248226478695869\n",
      "Running loss:  10.977493572235108\n",
      "Training set accuracy: 0.9495640270411968\n",
      "Running loss:  10.832866001129151\n",
      "Training set accuracy: 0.9558994088321924\n",
      "Running loss:  10.707572174072265\n",
      "Training set accuracy: 0.95419748313725\n",
      "Running loss:  10.331046009063721\n",
      "Training set accuracy: 0.9337305121123791\n",
      "Running loss:  9.7387225151062\n",
      "Training set accuracy: 0.9081865184009075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on dev set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set ....\n",
      "Validation loss: 8.798\n",
      "Validation set accuracy: 0.903\n",
      "Validation loss: 8.809\n",
      "Validation set accuracy: 0.892\n",
      "Validation loss: 8.785\n",
      "Validation set accuracy: 0.916\n",
      "Validation loss: 8.775\n",
      "Validation set accuracy: 0.927\n",
      "Validation loss: 8.823\n",
      "Validation set accuracy: 0.878\n",
      "Validation loss: 8.788\n",
      "Validation set accuracy: 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 10.428025245666504 - acc: 0.9163627695778142 -- Validation loss: 8.775415420532227 - acc: 0.9080489522644452\n",
      " 20%|████████████████▌                                                                  | 1/5 [05:15<21:02, 315.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.775\n",
      "Validation set accuracy: 0.926\n",
      "Finished Training\n",
      "Running EPOCH 2\n",
      "Running loss:  8.834084033966064\n",
      "Training set accuracy: 0.8863781534135342\n",
      "Running loss:  8.493098640441895\n",
      "Training set accuracy: 0.9065636806190014\n",
      "Running loss:  8.321365928649902\n",
      "Training set accuracy: 0.9294439375400543\n",
      "Running loss:  8.1313325881958\n",
      "Training set accuracy: 0.9536877188831567\n",
      "Running loss:  8.084994792938232\n",
      "Training set accuracy: 0.9588215101510287\n",
      "Running loss:  8.060502433776856\n",
      "Training set accuracy: 0.9607950186356902\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on dev set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set ....\n",
      "Validation loss: 8.105\n",
      "Validation set accuracy: 0.904\n",
      "Validation loss: 8.123\n",
      "Validation set accuracy: 0.886\n",
      "Validation loss: 8.085\n",
      "Validation set accuracy: 0.925\n",
      "Validation loss: 8.083\n",
      "Validation set accuracy: 0.927\n",
      "Validation loss: 8.127\n",
      "Validation set accuracy: 0.882\n",
      "Validation loss: 8.096\n",
      "Validation set accuracy: 0.913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 8.285820007324219 - acc: 0.9364562571534644 -- Validation loss: 8.081256866455078 - acc: 0.909367635846138\n",
      " 40%|█████████████████████████████████▏                                                 | 2/5 [11:19<16:30, 330.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.081\n",
      "Validation set accuracy: 0.928\n",
      "Finished Training\n",
      "Running EPOCH 3\n",
      "Running loss:  8.055578136444092\n",
      "Training set accuracy: 0.9558352651074529\n",
      "Running loss:  8.05319766998291\n",
      "Training set accuracy: 0.9537899188697339\n",
      "Running loss:  8.049210262298583\n",
      "Training set accuracy: 0.954549840092659\n",
      "Running loss:  8.0414888381958\n",
      "Training set accuracy: 0.9627291116863489\n",
      "Running loss:  8.03413667678833\n",
      "Training set accuracy: 0.9681146085262299\n",
      "Running loss:  8.031125259399413\n",
      "Training set accuracy: 0.9706734491512179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on dev set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set ....\n",
      "Validation loss: 8.061\n",
      "Validation set accuracy: 0.939\n",
      "Validation loss: 8.070\n",
      "Validation set accuracy: 0.931\n",
      "Validation loss: 8.050\n",
      "Validation set accuracy: 0.951\n",
      "Validation loss: 8.050\n",
      "Validation set accuracy: 0.951\n",
      "Validation loss: 8.063\n",
      "Validation set accuracy: 0.938\n",
      "Validation loss: 8.063\n",
      "Validation set accuracy: 0.938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 8.042633056640625 - acc: 0.9619960479060377 -- Validation loss: 8.044320106506348 - acc: 0.9436368202524525\n",
      " 60%|█████████████████████████████████████████████████▊                                 | 3/5 [17:04<11:08, 334.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.044\n",
      "Validation set accuracy: 0.956\n",
      "Finished Training\n",
      "Running EPOCH 4\n",
      "Running loss:  8.031366634368897\n",
      "Training set accuracy: 0.9700438816100359\n",
      "Running loss:  8.027882194519043\n",
      "Training set accuracy: 0.9734340833500028\n",
      "Running loss:  8.028804492950439\n",
      "Training set accuracy: 0.972716435790062\n",
      "Running loss:  8.027166175842286\n",
      "Training set accuracy: 0.9742087926715612\n",
      "Running loss:  8.033147144317628\n",
      "Training set accuracy: 0.9683932859450579\n",
      "Running loss:  8.031132793426513\n",
      "Training set accuracy: 0.9710642091929913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on dev set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set ....\n",
      "Validation loss: 8.036\n",
      "Validation set accuracy: 0.965\n",
      "Validation loss: 8.037\n",
      "Validation set accuracy: 0.963\n",
      "Validation loss: 8.034\n",
      "Validation set accuracy: 0.967\n",
      "Validation loss: 8.046\n",
      "Validation set accuracy: 0.955\n",
      "Validation loss: 8.041\n",
      "Validation set accuracy: 0.959\n",
      "Validation loss: 8.039\n",
      "Validation set accuracy: 0.962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:new model saved\n",
      "INFO:root:Train loss: 8.029426574707031 - acc: 0.9721133809374727 -- Validation loss: 8.02437686920166 - acc: 0.9636766527380262\n",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 4/5 [21:55<05:21, 321.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.024\n",
      "Validation set accuracy: 0.976\n",
      "Finished Training\n",
      "Running EPOCH 5\n",
      "Running loss:  8.023860836029053\n",
      "Training set accuracy: 0.9777472160756588\n",
      "Running loss:  8.023838233947753\n",
      "Training set accuracy: 0.9770548658445477\n",
      "Running loss:  8.024714374542237\n",
      "Training set accuracy: 0.9762678811326623\n",
      "Running loss:  8.024611854553223\n",
      "Training set accuracy: 0.9766347996890545\n",
      "Running loss:  8.025888919830322\n",
      "Training set accuracy: 0.975245013460517\n",
      "Running loss:  8.024997901916503\n",
      "Training set accuracy: 0.976062455959618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on dev set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating validation set ....\n",
      "Validation loss: 8.039\n",
      "Validation set accuracy: 0.961\n",
      "Validation loss: 8.048\n",
      "Validation set accuracy: 0.953\n",
      "Validation loss: 8.042\n",
      "Validation set accuracy: 0.958\n",
      "Validation loss: 8.041\n",
      "Validation set accuracy: 0.960\n",
      "Validation loss: 8.050\n",
      "Validation set accuracy: 0.951\n",
      "Validation loss: 8.045\n",
      "Validation set accuracy: 0.955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Train loss: 8.024439811706543 - acc: 0.9766973451427792 -- Validation loss: 8.034225463867188 - acc: 0.9577606873852866\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 5/5 [26:34<00:00, 318.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation loss: 8.034\n",
      "Validation set accuracy: 0.966\n",
      "Finished Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "reload(train)\n",
    "from train import train_model\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "optimizer = torch.optim.Adam(siamese_lstm_attention_model.parameters(), lr=learning_rate, betas=(0.9, 0.98))\n",
    "\n",
    "siamese_lstm_attention = train_model(\n",
    "    model=siamese_lstm_attention_model,\n",
    "    optimizer=optimizer,\n",
    "    dataloader=sick_dataloaders,\n",
    "    data=sick_data,\n",
    "    max_epochs=max_epochs,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fe9cef",
   "metadata": {},
   "source": [
    "## Part 4. Evaluation and Analysis (2 points)  \n",
    "Implement function evaluate_test_set to calculate the final accuracy of the performance evaluation metric on the test data.  \n",
    "Compare the result with the original AAAI paper. Сomment on effect of penalty loss on model capacity. Did the inclusion of the self-attention block improve the results? If yes, then how? Can you think of additional techniques to improve the results? Briefly answer these questions in the markdown cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "143154f6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Evaluating accuracy on test set\n",
      "INFO:root:Evaluating accuracy on test set\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished testing..............\n",
      "Total test set accuracy: 0.576\n"
     ]
    }
   ],
   "source": [
    "reload(test)\n",
    "evaluate_test_set(\n",
    "    model=siamese_lstm_attention,\n",
    "    data_loader=sick_dataloaders,\n",
    "    config_dict={\n",
    "        \"device\": device,\n",
    "        \"model_name\": \"siamese_lstm_attention\",\n",
    "        \"self_attention_config\": self_attention_config,\n",
    "    },\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ef76c",
   "metadata": {},
   "source": [
    "### Part 4 Answers\n",
    "\n",
    "We are getting an MSE of (1 - 0.576) = 0.424, whereas in the AAAI paper, the MSE is 0.2286. Hence, we do not achieve a better result only by addition of self-attention block. Note: this may be due to the fact that in the paper different techniques of regularisation are used as compared to what we used. In general, we expect the self-attention block to improve results as it will assign higher importance to words inferred from context i.e. semantically similar word pairs will have higher weights.\n",
    "\n",
    "The penalty term acts as a regularisation term for the self-attention block in the context of the MSE loss function and prevents the model to some extent from overfitting. Without the penalty term, the self-attention block may put a lot of weight on words that are not very relevant to the context. In our analysis we observed that the performance/accuracy increases upon addition of penalty term, i.e. final accuracy is higher when penalty term of 1 is included.\n",
    "\n",
    "To improve the results, various other forms of regularisation and initialization techniques can be used for eg. initialisation of hidden states of the BiLSTM with random Gaussian entries. \n",
    "Also training can be increased over multiple epochs, and early stopping can be employed to get the best hyperparameter combination that results in a lower validation set error. Also additional dropout layers can be added to prevent the model from overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0612cb2",
   "metadata": {},
   "source": [
    "### Code References:\n",
    "\n",
    "1. Self-attentive sentence embeddings (for Task 1 model reference): https://github.com/prakashpandey9/Text-Classification-Pytorch\n",
    "2.  Self-attentive sentence embeddings (for Task 1 utility functions reference i.e. frobenius norm calculation etc.): https://github.com/kaushalshetty/Structured-Self-Attention\n",
    "3. HuggingFace - SICK dataset loading\n",
    "\n",
    "### Extra libraries used:\n",
    "1. Gensim - for easy stopword removal\n",
    "2. HuggingFace - to load the SICK dataset directly with the official train, valid, test partitions\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
